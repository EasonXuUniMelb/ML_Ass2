{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP30027 Machine Learning Project 2\n",
    "\n",
    "## Description of text features\n",
    "\n",
    "This notebook describes the pre-computed text features provided for Project 2. **You do not need to recompute the features yourself for this assignment** -- this information is just for your reference. However, feel free to experiment with different text features if you are interested. If you do want to try generating your own text features, some things to keep in mind:\n",
    "- There are many different decisions you can make throughout the feature design process, from the text preprocessing to the size of the output vectors. There's no guarantee that the defaults we chose will produce the best possible text features for this classification task, so feel free to experiment with different settings.\n",
    "- These features must be trained using a training corpus. Generally, the training corpus should not include validation samples, but for the purposes of this assignment we have used the entire non-test set (training+validation) as the training corpus, to allow you to experiment with different validation sets. If you recompute the text features as part of your own model, you should exclude validation samples and compute them on training samples only. For example, if you do N-fold cross-validation, this means generating N sets of features for N different training-validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       Best of Dr Jean: Reading & Writing\n",
       "1                                      Here All Dwell Free\n",
       "2                                    Boomer's Big Surprise\n",
       "3        I'll Go and Do More: Annie Dodge Wauneka, Nava...\n",
       "4                                                       Us\n",
       "                               ...                        \n",
       "23058                                   Black Coffee Blues\n",
       "23059          America's Champion Swimmer: Gertrude Ederle\n",
       "23060                   Crime and Custom in Savage Society\n",
       "23061    The Name and Nature of Poetry and Other Select...\n",
       "23062                              Redemption (Sevens, #7)\n",
       "Name: Name, Length: 23063, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# read text\n",
    "# for DEMONSTRATION PURPOSES, the entire training set will be used to train the models and also as a test set\n",
    "x_train_original = pd.read_csv(r\"book_rating_train.csv\", index_col = False, delimiter = ',', header=0)\n",
    "# use recipe name as an example\n",
    "train_corpus_name = x_train_original['Name']\n",
    "test_name = x_train_original['Name']\n",
    "\n",
    "test_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count vectorizer\n",
    "\n",
    "A count vectorizer converts documents to vectors which represent word counts. Each column in the output represents a different word and the values indicate the number of times that word appeared in the document. The overall size of a count vector matrix can be quite large (the number of columns is the total number of different words used across all documents in a corpus), but most entries in the matrix are zero (each document contains only a few of all the possible words). Therefore, it is most efficient to represent the count vectors as a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20766\n",
      "(23063, 20766)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# preprocess text and compute counts\n",
    "vocab_name = CountVectorizer(stop_words='english').fit(train_corpus_name)\n",
    "\n",
    "# generate counts for a new set of documents\n",
    "x_train_name = vocab_name.transform(train_corpus_name)\n",
    "x_test_name = vocab_name.transform(test_name)\n",
    "\n",
    "# check the number of words in vocabulary\n",
    "print(len(vocab_name.vocabulary_))\n",
    "# check the shape of sparse matrix\n",
    "print(x_train_name.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec\n",
    "\n",
    "doc2vec methods are an extension of word2vec. word2vec maps words to a high-dimensional vector space in such a way that words which appear in similar contexts will be close together in the space. doc2vec does a similar embedding for multi-word passages. The doc2vec (or Paragraph Vector) method was introduced by:\n",
    "\n",
    "**Le & Mikolov (2014)** Distributed Representations of Sentences and Documents<br>\n",
    "https://arxiv.org/pdf/1405.4053v2.pdf\n",
    "\n",
    "The implementation of doc2vec used for this project is from gensim and documented here:<br>\n",
    "https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "\n",
    "The size of the output vector is a free parameter. Most implemementations use around 100-300 dimensions, but the best size depends on the problem you're trying to solve with the embeddings and the number of training samples, so you may wish to try different vector sizes. We provided doc2vec features for Name (vec_size = 100), Authors (vec_size = 20) and Description (vec_size = 100). The vectors themselves represent directions in a high-dimensional concept space; the columns do not represent specific words or phrases. Values in the vector are continuous real numbers and can be negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23063, 100)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "# size of the output vector\n",
    "vec_size = 100\n",
    "\n",
    "# function to preprocess and tokenize text\n",
    "def tokenize_corpus(txt, tokens_only=False):\n",
    "    for i, line in enumerate(txt):\n",
    "        tokens = gensim.utils.simple_preprocess(line)\n",
    "        if tokens_only:\n",
    "            yield tokens\n",
    "        else:\n",
    "            yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "\n",
    "# tokenize a training corpus\n",
    "corpus_name = list(tokenize_corpus(train_corpus_name))\n",
    "\n",
    "# train doc2vec on the training corpus\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=vec_size, min_count=2, epochs=40)\n",
    "model.build_vocab(corpus_name)\n",
    "model.train(corpus_name, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# tokenize new documents\n",
    "doc = list(tokenize_corpus(test_name, tokens_only=True))\n",
    "\n",
    "# generate embeddings for the new documents\n",
    "x_test_name = np.zeros((len(doc),vec_size))\n",
    "for i in range(len(doc)):\n",
    "    x_test_name[i,:] = model.infer_vector(doc[i])\n",
    "    \n",
    "# check the shape of doc_emb\n",
    "print(x_test_name.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03907779, -0.20084597, -0.12767211,  0.27530372,  0.0062864 ,\n",
       "       -0.13608879,  0.17285869, -0.02914329,  0.05223615, -0.03201814,\n",
       "       -0.06710566, -0.07091709, -0.21743639,  0.32974994,  0.11680586,\n",
       "        0.11761668, -0.02285548, -0.13967532, -0.12643696,  0.10689881,\n",
       "        0.20067956,  0.21798152,  0.06642779,  0.09075683,  0.04523761,\n",
       "        0.017419  , -0.04154637, -0.0230053 , -0.12487628, -0.12327845,\n",
       "        0.31904334,  0.11371993,  0.04002243, -0.05300687, -0.15423736,\n",
       "        0.06090686,  0.15133564,  0.11005343,  0.10606996,  0.19500077,\n",
       "       -0.30476153, -0.15618828, -0.10656201, -0.01500529,  0.15779527,\n",
       "        0.10920191, -0.04424483,  0.0214396 ,  0.18373054,  0.00571012,\n",
       "        0.06760812, -0.13702995, -0.28596723, -0.1529157 , -0.02016098,\n",
       "        0.02721264, -0.00305189,  0.03771792,  0.03245747, -0.05821702,\n",
       "        0.10928577, -0.01517053, -0.04586982, -0.04600242,  0.18858071,\n",
       "        0.17858487, -0.08391981, -0.15932426, -0.07839748, -0.06234469,\n",
       "       -0.12358258,  0.11616421, -0.41674551, -0.21191762,  0.11816263,\n",
       "       -0.09421347,  0.09371163,  0.03096224,  0.02938019, -0.11735602,\n",
       "        0.06241326,  0.20773821, -0.02883309,  0.15159453, -0.15660049,\n",
       "       -0.10748424,  0.03615046,  0.04706481,  0.05038706,  0.0469456 ,\n",
       "       -0.04741086,  0.18672661,  0.04019495,  0.04602727,  0.06646553,\n",
       "        0.06985315,  0.14073367, -0.21890724, -0.04854722,  0.14100423])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_name[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "09e83f04e46517ea1a03852b22098722c60f5ebfc3f611b0edf7286ad5c15192"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
